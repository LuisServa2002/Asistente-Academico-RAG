# ü™ü Gu√≠a de Ejecuci√≥n - Windows (VSCode)

Esta gu√≠a te ayudar√° a ejecutar el Asistente Acad√©mico con RAG en Windows usando Visual Studio Code.

---

## üìã Prerrequisitos

### 1. Python 3.10 o superior

**Verificar instalaci√≥n:**
```powershell
python --version
```

**Si no est√° instalado:**
- Descargar desde: https://www.python.org/downloads/
- ‚ö†Ô∏è **IMPORTANTE**: Marcar la opci√≥n "Add Python to PATH" durante la instalaci√≥n

### 2. Visual Studio Code

**Instalar VSCode:**
- Descargar desde: https://code.visualstudio.com/
- Instalar extensi√≥n "Python" desde el marketplace

### 3. Ollama (Modelo LLaMA)

**Instalaci√≥n en Windows:**

1. **Descargar Ollama:**
   - Visitar: https://ollama.com/download
   - Descargar el instalador para Windows
   - Ejecutar el instalador

2. **Verificar instalaci√≥n:**
   ```powershell
   ollama --version
   ```

3. **Descargar modelo LLaMA:**
   ```powershell
   ollama pull llama2:7b
   ```
   
   ‚ö†Ô∏è **Nota**: Este proceso puede tardar varios minutos y requiere ~4GB de espacio libre.

4. **Verificar modelo descargado:**
   ```powershell
   ollama list
   ```

5. **Probar modelo:**
   ```powershell
   ollama run llama2:7b "Hola, ¬øc√≥mo est√°s?"
   ```

---

## üöÄ Configuraci√≥n del Proyecto

### Paso 1: Abrir el proyecto en VSCode

1. Abrir Visual Studio Code
2. File ‚Üí Open Folder ‚Üí Seleccionar la carpeta del proyecto
3. Asegurarse de que VSCode detecta Python (ver esquina inferior derecha)

### Paso 2: Crear entorno virtual

**En la terminal integrada de VSCode (Terminal ‚Üí New Terminal):**

```powershell
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
.\venv\Scripts\Activate.ps1
```

‚ö†Ô∏è **Si aparece error de ejecuci√≥n de scripts:**
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

**Alternativa (si PowerShell no funciona):**
```cmd
venv\Scripts\activate.bat
```

### Paso 3: Instalar dependencias

```powershell
# Actualizar pip
python -m pip install --upgrade pip

# Instalar dependencias del proyecto
pip install -r requirements.txt
```

**Verificar instalaci√≥n:**
```powershell
pip list
```

Deber√≠as ver: `langchain`, `chromadb`, `streamlit`, `sentence-transformers`, etc.

---

## üéØ Ejecuci√≥n del Sistema

### Opci√≥n 1: Interfaz Streamlit (Recomendado)

**1. Verificar que Ollama est√© corriendo:**

Abre una nueva terminal y ejecuta:
```powershell
ollama serve
```

‚ö†Ô∏è **IMPORTANTE**: Deja esta terminal abierta. Ollama debe estar corriendo en segundo plano.

**2. En otra terminal (con el entorno virtual activado):**

```powershell
# Aseg√∫rate de estar en el directorio del proyecto
cd "C:\Users\luisa\Desktop\UNI\7-CICLO\IA\PC's\PC 05\asistente-academico-rag"

# Activar entorno virtual si no est√° activo
.\venv\Scripts\Activate.ps1

# Ejecutar Streamlit
streamlit cache clear
streamlit run app.py
```

**3. Abrir en navegador:**

- Streamlit abrir√° autom√°ticamente: `http://localhost:8501`
- Si no se abre, copiar la URL que aparece en la terminal

**4. Usar la aplicaci√≥n:**

- Subir PDFs acad√©micos en la barra lateral
- Configurar modelo, temperatura y top-k
- Clic en "üöÄ Inicializar Asistente"
- Esperar a que procese los documentos (puede tardar varios minutos)
- ¬°Hacer preguntas!

### Opci√≥n 2: Script Python directo

**1. Preparar documentos:**

Colocar PDFs en la carpeta `documentos/`:
```
documentos/
  ‚îú‚îÄ‚îÄ apuntes_ia.pdf
  ‚îî‚îÄ‚îÄ libro_ml.pdf
```

**2. Ejecutar script:**

```powershell
# Con entorno virtual activado
python asistente.py
```

**3. Modificar el script:**

Editar `asistente.py` (l√≠neas 178-183) para especificar tus PDFs:
```python
pdfs = [
    "documentos/tu_documento1.pdf",
    "documentos/tu_documento2.pdf",
]
```

---

## üîß Soluci√≥n de Problemas Comunes

### Error: "Ollama no se encuentra"

**Soluci√≥n:**
```powershell
# Verificar que Ollama est√© en PATH
ollama --version

# Si no funciona, agregar manualmente al PATH:
# 1. Buscar "Variables de entorno" en Windows
# 2. Agregar ruta de Ollama (normalmente: C:\Users\<usuario>\AppData\Local\Programs\Ollama)
```

### Error: "No se puede conectar a Ollama"

**Soluci√≥n:**
```powershell
# Iniciar servidor Ollama manualmente
ollama serve

# En otra terminal, verificar:
curl http://localhost:11434/api/tags
```

### Error: "Modelo no encontrado"

**Soluci√≥n:**
```powershell
# Listar modelos disponibles
ollama list

# Si llama2:7b no est√°, descargarlo:
ollama pull llama2:7b

# Alternativas m√°s peque√±as (si tienes poca RAM):
ollama pull llama2:7b-chat-q4_0  # Versi√≥n cuantizada (menos RAM)
```

### Error: "Out of memory" o sistema lento

**Soluciones:**
1. Usar modelo m√°s peque√±o:
   ```powershell
   ollama pull llama2:7b-chat-q4_0
   ```
2. Reducir tama√±o de chunks en `asistente.py`:
   ```python
   chunk_size=500  # En lugar de 1000
   ```
3. Procesar menos documentos a la vez

### Error: "ModuleNotFoundError"

**Soluci√≥n:**
```powershell
# Asegurarse de que el entorno virtual est√° activado
# Deber√≠as ver (venv) al inicio de la l√≠nea de comandos

# Reinstalar dependencias
pip install -r requirements.txt
```

### Error: "ChromaDB lock" o "Database locked"

**Soluci√≥n:**
```powershell
# Cerrar todas las instancias de Python/Streamlit
# Eliminar carpeta chroma_db si es necesario (se regenerar√°)
Remove-Item -Recurse -Force chroma_db
```

### Streamlit no se abre autom√°ticamente

**Soluci√≥n:**
- Copiar la URL que aparece en la terminal (ej: `http://localhost:8501`)
- Abrir manualmente en el navegador

---

## üìä Verificaci√≥n del Sistema

### Script de verificaci√≥n r√°pida

Ejecutar `verificar_setup.py`:
```powershell
python verificar_setup.py
```

Este script verificar√°:
- ‚úÖ Python instalado
- ‚úÖ Ollama instalado y corriendo
- ‚úÖ Modelo LLaMA disponible
- ‚úÖ Dependencias instaladas
- ‚úÖ Estructura de carpetas correcta

---

## üéì Flujo de Trabajo Recomendado

### Primera vez:

1. ‚úÖ Instalar Ollama y descargar modelo
2. ‚úÖ Crear entorno virtual e instalar dependencias
3. ‚úÖ Colocar PDFs en carpeta `documentos/`
4. ‚úÖ Ejecutar `streamlit run app.py`
5. ‚úÖ Subir PDFs y hacer clic en "Inicializar Asistente"
6. ‚úÖ Esperar procesamiento (primera vez puede tardar)
7. ‚úÖ ¬°Hacer preguntas!

### Sesiones posteriores:

1. ‚úÖ Activar entorno virtual
2. ‚úÖ Iniciar Ollama (`ollama serve`)
3. ‚úÖ Ejecutar Streamlit
4. ‚úÖ Opci√≥n A: Subir nuevos PDFs
5. ‚úÖ Opci√≥n B: Clic en "Cargar Base de Datos Existente" (m√°s r√°pido)

---

## üìù Notas Importantes

### Rendimiento:

- **Primera carga de documentos**: Puede tardar 5-15 minutos dependiendo del tama√±o
- **Consultas**: 5-30 segundos por pregunta (depende del hardware)
- **RAM recomendada**: M√≠nimo 8GB, ideal 16GB

### Almacenamiento:

- **Modelo LLaMA**: ~4GB
- **Base de datos vectorial**: ~100-500MB por documento (depende del tama√±o)
- **Espacio total recomendado**: ~10GB libres

### Mejores Pr√°cticas:

1. **Procesar documentos una vez**: La base de datos se guarda en `chroma_db/`
2. **Reutilizar base de datos**: Usar "Cargar Base de Datos Existente" en sesiones posteriores
3. **PDFs peque√±os**: Para pruebas, usar documentos de 10-50 p√°ginas
4. **Guardar trabajo**: La base de datos se persiste autom√°ticamente

---

## üÜò Soporte Adicional

Si encuentras problemas:

1. Revisar logs en la terminal de VSCode
2. Verificar que Ollama est√© corriendo: `ollama list`
3. Probar modelo directamente: `ollama run llama2:7b "test"`
4. Verificar versi√≥n de Python: `python --version` (debe ser 3.10+)
5. Revisar `requirements.txt` y versiones instaladas: `pip list`

---

## ‚úÖ Checklist de Verificaci√≥n

Antes de ejecutar, verifica:

- [ ] Python 3.10+ instalado
- [ ] VSCode instalado con extensi√≥n Python
- [ ] Ollama instalado y funcionando
- [ ] Modelo LLaMA descargado (`ollama list`)
- [ ] Entorno virtual creado y activado
- [ ] Dependencias instaladas (`pip list`)
- [ ] Ollama corriendo (`ollama serve`)
- [ ] PDFs en carpeta `documentos/` (opcional para primera ejecuci√≥n)

---

**¬°Listo para comenzar! üöÄ**

Si tienes dudas, revisa la secci√≥n de "Soluci√≥n de Problemas" o consulta el README.md principal.


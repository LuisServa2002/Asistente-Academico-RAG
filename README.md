# Asistente AcadÃ©mico con RAG ğŸ“

Sistema de asistente acadÃ©mico basado en Retrieval-Augmented Generation (RAG) que responde preguntas sobre documentos acadÃ©micos usando LLaMA y bÃºsqueda vectorial.

## ğŸ‘¥ Equipo

- Iman Noriega Melissa (20224041G)
- Trujillo Serva Luis Andre (20220428D)
- Orrego TorrejÃ³n Diego A. (20204161G)
- MÃ©ndez Gonzalo Miguel (20220264A)
- Pineda GarcÃ­a Diego (20222117F)

**Universidad Nacional de IngenierÃ­a**  
Facultad de Ciencias - Ciencia de la ComputaciÃ³n  
Curso: Inteligencia Artificial - 2025-2

---

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.10+
- Ollama instalado (para LLaMA local)
- 8GB+ RAM recomendado
- ~10GB espacio libre en disco

### InstalaciÃ³n RÃ¡pida

**Para Windows (VSCode):** Ver guÃ­a detallada en [`GUIA_EJECUCION_WINDOWS.md`](GUIA_EJECUCION_WINDOWS.md)

**Pasos generales:**

1. **Instalar Ollama:**
   - Windows: Descargar desde https://ollama.com/download
   - Linux/Mac: `curl -fsSL https://ollama.com/install.sh | sh`
   - Descargar modelo: `ollama pull llama2:7b`

2. **Configurar proyecto:**
   ```bash
   # Crear entorno virtual
   python -m venv venv
   
   # Activar entorno virtual
   # Windows:
   venv\Scripts\activate
   # Linux/Mac:
   source venv/bin/activate
   
   # Instalar dependencias
   pip install -r requirements.txt
   ```

3. **Verificar instalaciÃ³n:**
   ```bash
   python verificar_setup.py
   ```

---

## ğŸ“– Uso

### OpciÃ³n 1: Interfaz Streamlit (Recomendado) â­

**1. Iniciar servidor Ollama (en terminal separada):**
```bash
ollama serve
```

**2. Ejecutar aplicaciÃ³n:**
```bash
streamlit run app.py
```

**3. Usar la aplicaciÃ³n:**
- Abre `http://localhost:8501` en tu navegador
- Sube PDFs acadÃ©micos en la barra lateral
- Configura modelo, temperatura y top-k
- Haz clic en "ğŸš€ Inicializar Asistente"
- Espera el procesamiento (primera vez puede tardar)
- Â¡Comienza a hacer preguntas!

**4. Sesiones posteriores:**
- Usa "ğŸ“‚ Cargar Base de Datos Existente" para cargar documentos ya procesados (mÃ¡s rÃ¡pido)

### OpciÃ³n 2: Script Python

```python
from asistente import AsistenteAcademico

# Inicializar con parÃ¡metros personalizados
asistente = AsistenteAcademico(
    modelo_llama="llama2:7b",
    temperatura=0.3,
    top_k=3
)

# Cargar documentos (primera vez)
asistente.cargar_documentos([
    "documentos/apuntes_ia.pdf",
    "documentos/libro_ml.pdf"
])

# O cargar base de datos existente (mÃ¡s rÃ¡pido)
# asistente.cargar_vectorstore_existente()

# Consultar
resultado = asistente.consultar("Â¿QuÃ© es RAG?")
print(resultado["respuesta"])

# Ver fuentes
asistente.mostrar_fuentes(resultado["fuentes"])
```

---

## ğŸ“ Estructura del Proyecto

```
Proyecto-IA/
â”œâ”€â”€ app.py                      # Interfaz Streamlit
â”œâ”€â”€ asistente.py               # Clase principal del RAG
â”œâ”€â”€ requirements.txt           # Dependencias
â”œâ”€â”€ README.md                  # Este archivo
â”‚
â”œâ”€â”€ documentos/                # PDFs de entrada
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ chroma_db/                 # Base de datos vectorial (auto-generado)
â”‚
â”œâ”€â”€ evaluacion/               
â”‚   â”œâ”€â”€ metricas.py           # Scripts de evaluaciÃ³n
â”‚   â””â”€â”€ test_preguntas.json   # Conjunto de pruebas
â”‚
â””â”€â”€ notebooks/
    â””â”€â”€ experimentacion.ipynb  # Jupyter para pruebas
```

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **LLM**: LLaMA 2/3 (vÃ­a Ollama)
- **Embeddings**: sentence-transformers (paraphrase-multilingual-MiniLM-L12-v2)
- **Vector DB**: ChromaDB
- **Framework**: LangChain
- **Interfaz**: Streamlit
- **PDF Processing**: PyPDF2

---

## ğŸ“Š EvaluaciÃ³n

Ejecutar evaluaciones automÃ¡ticas:

```bash
python -c "from evaluacion.metricas import EvaluadorAsistente; from asistente import AsistenteAcademico; a = AsistenteAcademico(); a.cargar_vectorstore_existente(); e = EvaluadorAsistente(a); r = e.evaluar_conjunto_completo(e.cargar_conjunto_prueba('evaluacion/test_preguntas.json')); e.generar_reporte(r)"
```

MÃ©tricas incluidas:
- ROUGE (calidad de generaciÃ³n)
- Latencia de respuesta
- EvaluaciÃ³n manual de relevancia

---

## âš™ï¸ ConfiguraciÃ³n

### ParÃ¡metros Configurables

**En la interfaz Streamlit:**
- **Modelo**: llama2:7b, llama3:8b, mistral:7b
- **Temperatura**: Control de creatividad (0.0 - 1.0)
  - 0.0-0.3: Respuestas mÃ¡s determinÃ­sticas y precisas
  - 0.4-0.7: Balance entre precisiÃ³n y creatividad
  - 0.8-1.0: Respuestas mÃ¡s creativas y variadas
- **Top-K**: NÃºmero de fragmentos a recuperar (1 - 10)
  - Menor valor: Respuestas mÃ¡s especÃ­ficas pero menos contexto
  - Mayor valor: MÃ¡s contexto pero puede incluir informaciÃ³n menos relevante

**En cÃ³digo Python:**
```python
# Actualizar parÃ¡metros dinÃ¡micamente
asistente.actualizar_parametros(temperatura=0.5, top_k=5)
```

---

## ğŸ” Arquitectura RAG

```
1. IndexaciÃ³n:
   Documentos â†’ Chunking â†’ Embeddings â†’ ChromaDB

2. Consulta:
   Pregunta â†’ BÃºsqueda SemÃ¡ntica â†’ Contexto â†’ LLaMA â†’ Respuesta
```

---

## âš ï¸ Limitaciones Conocidas

- Requiere Ollama instalado localmente
- Modelos LLaMA necesitan ~8GB RAM mÃ­nimo
- Respuestas limitadas al contenido de los documentos cargados
- No guarda historial entre sesiones

---

## ğŸ¤ Consideraciones Ã‰ticas

- Las respuestas estÃ¡n limitadas al contenido de los documentos
- Se muestran fuentes para verificar informaciÃ³n
- Sistema indica cuando no tiene suficiente informaciÃ³n
- Respeta derechos de autor (solo documentos propios)

---

## ğŸ“ Licencia

Proyecto acadÃ©mico - Universidad Nacional de IngenierÃ­a

---

## ğŸ“§ Contacto

Coordinador: Luis Andre Trujillo Serva  
Email: luis.trujillo.s@uni.edu.pe
